{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Name\": string  // Extract the full name from the resume text.\n",
      "\t\"Mail ID\": string  // Extract the email ID from the resume text.\n",
      "\t\"LinkedIn\": string  // Extract the LinkedIn profile URL from the resume text.\n",
      "\t\"Work Experience\": string  // Extract all organization names where the person has worked, along with the number of years or months worked there and the designations held, and output them as a comma-separated Python list.\n",
      "\t\"Company Details\": string  // Extract all company names and details where the person has worked, and output them as a comma-separated Python list.\n",
      "\t\"Technical Skills\": string  // Extract all technical skills mentioned in the resume text and output them as a comma-separated Python list.\n",
      "\t\"Soft Skills\": string  // Extract all soft skills mentioned in the resume text and output them as a comma-separated Python list.\n",
      "\t\"Projects\": string  // Extract all project titles mentioned in the resume text and output them as a comma-separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define individual schemas for each resume detail\n",
    "name_schema = ResponseSchema(name=\"Name\",\n",
    "                             description=\"Extract the full name from the resume text.\")\n",
    "\n",
    "mailid_schema = ResponseSchema(name=\"Mail ID\",\n",
    "                               description=\"Extract the email ID from the resume text.\")\n",
    "\n",
    "linkedin_schema = ResponseSchema(name=\"LinkedIn\",\n",
    "                                 description=\"Extract the LinkedIn profile URL from the resume text.\")\n",
    "\n",
    "workexp_schema = ResponseSchema(name=\"Work Experience\",\n",
    "                                description=\"Extract all organization names where the person has worked, along with the number of years or months worked there and the designations held, and output them as a comma-separated Python list.\")\n",
    "\n",
    "companydetails_schema = ResponseSchema(name=\"Company Details\",\n",
    "                                       description=\"Extract all company names and details where the person has worked, and output them as a comma-separated Python list.\")\n",
    "\n",
    "technical_skills_schema = ResponseSchema(name=\"Technical Skills\",\n",
    "                                         description=\"Extract all technical skills mentioned in the resume text and output them as a comma-separated Python list.\")\n",
    "\n",
    "soft_skills_schema = ResponseSchema(name=\"Soft Skills\",\n",
    "                                    description=\"Extract all soft skills mentioned in the resume text and output them as a comma-separated Python list.\")\n",
    "\n",
    "projects_schema = ResponseSchema(name=\"Projects\",\n",
    "                                 description=\"Extract all project titles mentioned in the resume text and output them as a comma-separated Python list.\")\n",
    "\n",
    "# Combine all schemas into a list\n",
    "response_schemas = [\n",
    "    name_schema,\n",
    "    mailid_schema,\n",
    "    linkedin_schema,\n",
    "    workexp_schema,\n",
    "    companydetails_schema,\n",
    "    technical_skills_schema,\n",
    "    soft_skills_schema,\n",
    "    projects_schema\n",
    "]\n",
    "\n",
    "# Create the output parser using the response schemas\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Get the format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Charlotte Donald.docx ---\n",
      "```json\n",
      "{\n",
      "    \"Name\": \"Charlotte Donald\",\n",
      "    \"Mail ID\": \"CDonald@uk.ey.com\",\n",
      "    \"GitHub\": \"\",\n",
      "    \"LinkedIn\": \"\",\n",
      "    \"Work Experience\": \"EY, London. (Manager), September 2017 â€“Present\",\n",
      "    \"Company Details\": \"EY, London\",\n",
      "    \"Technical Skills\": \"\",\n",
      "    \"Soft Skills\": \"strong insurance technical knowledge, communication, project management, client relationships, leadership, problem-solving, time management, teamwork, adaptability, attention to detail\",\n",
      "    \"Projects\": \"\"\n",
      "}\n",
      "```\n",
      "--- Umang Purwar RESUME-de.pdf ---\n",
      "```json\n",
      "{\n",
      "    \"Name\": \"Umang Purwar\",\n",
      "    \"Mail ID\": \"umangpurwar03@gmail.com\",\n",
      "    \"GitHub\": \"\",\n",
      "    \"LinkedIn\": \"LinkedIn\",\n",
      "    \"Work Experience\": \"JR. DATA SCIENTIST at INNODATATICS, DATA SCIENCE INTERN at INNODATATICS\",\n",
      "    \"Company Details\": \"INNODATATICS\",\n",
      "    \"Technical Skills\": \"Python, data analysis, artificial intelligence, computer vision, machine learning, NLP, AWS, Power BI, deep learning frameworks, MLops, ETL, Web scrapping, Large Language model, Data Visualization, Neural Networks, CNN, YOLO, TensorFlow, Keras, PyTorch, NumPy, Pandas, Scikit-learn, OpenCV, statsmodels, Mage-AI ETL tool\",\n",
      "    \"Soft Skills\": \"\",\n",
      "    \"Projects\": \"Domain Chatbot, Your AI Assistant, Sales Tracking Dashboard for TMT Rods Across India, BAR-BENDING-SCHEDULING\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from groq import Groq\n",
    "# Path to the output CSV file\n",
    "csv_file = 'output.csv'\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a .docx file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import docx\n",
    "        doc = docx.Document(docx_path)\n",
    "        text = []\n",
    "        for para in doc.paragraphs:\n",
    "            text.append(para.text)\n",
    "        return '\\n'.join(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to extract text from {docx_path}: {e}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a .pdf file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "        pdf_text = []\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                pdf_text.append(page.extract_text())\n",
    "        return '\\n'.join(pdf_text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to extract text from {pdf_path}: {e}\")\n",
    "\n",
    "def extract_text_from_resume(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a resume file (.docx or .pdf).\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.docx'):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "def extract_text_from_resumes_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts text from all resume files (.docx or .pdf) in a folder.\n",
    "    \"\"\"\n",
    "    resumes_text = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.docx') or filename.endswith('.pdf'):\n",
    "            try:\n",
    "                text = extract_text_from_resume(file_path)\n",
    "                resumes_text[filename] = text\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract text from {filename}: {e}\")\n",
    "    return resumes_text\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'data'\n",
    "resumes_text = extract_text_from_resumes_in_folder(folder_path)\n",
    "all_rows = []\n",
    "\n",
    "for filename, text in resumes_text.items():\n",
    "    print(f\"--- {filename} ---\")\n",
    "    # Define a template for the prompt\n",
    "    prompt_template = f'''\n",
    "        You are an AI bot designed to act as a professional for parsing resumes.\n",
    "        You are given with resume and your job is to extract the following information from the resume just that dont give additional text in the begining and end just this info:\n",
    "        1. full name\n",
    "        2. email id\n",
    "        3. github portfolio\n",
    "        4. linkedin id\n",
    "        5. employment details\n",
    "        6. technical skills\n",
    "        7. soft skills\n",
    "        Give the extracted information\n",
    "        and this is resume{text} {format_instructions}\n",
    "        '''\n",
    "    \n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt_template,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "\n",
    "    response_content = chat_completion.choices[0].message.content\n",
    "    print(response_content)\n",
    "    # Extract the JSON part from the string\n",
    "    start_index = response_content.find('{')\n",
    "    end_index = response_content.rfind('}') + 1\n",
    "    json_part = response_content[start_index:end_index]\n",
    "    \n",
    "    # Parse JSON string\n",
    "    data = json.loads(json_part)\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    # Write to CSV\n",
    "    if not os.path.isfile(csv_file):\n",
    "        # Write header if the file does not exist\n",
    "        df.to_csv(csv_file, index=False, mode='w')\n",
    "    else:\n",
    "        # Append to the file without writing the header\n",
    "        df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
